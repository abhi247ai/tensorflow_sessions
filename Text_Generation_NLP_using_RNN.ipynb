{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Text Generation NLP using RNN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPm6AeQflL723zZjWTWO0lt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/abhi247ai/tensorflow_sessions/blob/main/Text_Generation_NLP_using_RNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jk9lLy6zMPXs"
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "from keras.preprocessing import sequence\n",
        "import keras\n",
        "import tensorflow as tf\n",
        "import os\n",
        "import numpy as np"
      ],
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wk4cfNn_maE0"
      },
      "source": [
        "**Loading Text File**\n",
        "\n",
        "Method : 1 from googe databse as shown and used (shakespeare.txt)\n",
        "\n",
        "Method2 : 2 using our own file.txt, by executiong the following code\n",
        "\n",
        "from.google.colab import files\n",
        "\n",
        "\n",
        "path_to_file = list(files.upload().keys())[0]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p0rtttHnlOnt"
      },
      "source": [
        "path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')"
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FLtHfe1ynWDS"
      },
      "source": [
        "** Read the file **"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rm-E3lkcnSij",
        "outputId": "3e89b51d-42e1-4f1c-c398-f8875779f3a7"
      },
      "source": [
        "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
        "\n",
        "print('Length of text: {} characters'. format(len(text)))\n",
        "\n",
        "print(text[:250])"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of text: 1115394 characters\n",
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you know Caius Marcius is chief enemy to the people.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UIvUsltaqAOV"
      },
      "source": [
        "**Text-to-Integer: Encoding**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DyCML6DdnPO3"
      },
      "source": [
        "vocab = sorted(set(text))\n",
        "\n",
        "char2idx = {u:i for i, u in enumerate(vocab)}\n",
        "idx2char = np.array(vocab)\n",
        "\n",
        "def text_to_int(text):\n",
        "  return np.array([char2idx[c] for c in text])\n",
        "\n",
        "text_as_int = text_to_int(text)"
      ],
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-qUG76EVup4M",
        "outputId": "b6dd9d51-3bef-48a6-c795-a94725afe7cf"
      },
      "source": [
        "print('Text:', text[:13])\n",
        "print('Encoded:', text_to_int(text[:13]))\n"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text: First Citizen\n",
            "Encoded: [18 47 56 57 58  1 15 47 58 47 64 43 52]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h8XYvYUIvsuA",
        "outputId": "aaa72e06-4764-40e3-a255-2d5a88b0f156"
      },
      "source": [
        "# function for converting int to text\n",
        "\n",
        "def int_to_text(ints):\n",
        "  try:\n",
        "    ints = ints.numpy()\n",
        "  except:\n",
        "    pass\n",
        "  return ''.join(idx2char[ints])\n",
        "\n",
        "print(int_to_text(text_as_int[:13]))"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First Citizen\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2nDNPiSWxfEW"
      },
      "source": [
        "#creating sequence of text data\n",
        "\n",
        "seq_length = 100\n",
        "examples_per_epoch = len(text)//(seq_length+1)\n",
        "\n",
        "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)"
      ],
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hX-Ds3u1yG8z"
      },
      "source": [
        "# now make the batches of the sequences\n",
        "\n",
        "sequences = char_dataset.batch(seq_length+1, drop_remainder = True)"
      ],
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8xXrNRhkzN-7"
      },
      "source": [
        "Now, sequnce length of 101 is splitted in to input and output"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3IRFNbcEzXNF"
      },
      "source": [
        "def split_input_target(chunk):    #chunk is hello\n",
        "  input_text = chunk[:-1] #hell\n",
        "  target_text = chunk[1:] #ello\n",
        "  return input_text, target_text   #hell, #ello\n",
        "\n",
        "dataset = sequences.map(split_input_target)  #it applies every entry this function"
      ],
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YGfsfzZm0W80",
        "outputId": "73aa9d3e-4b20-4907-ca97-71d06186684f"
      },
      "source": [
        "for x,y in dataset.take(2):\n",
        "  print(\"\\n\\nExample\\n\")\n",
        "  print(\"INPUT\")\n",
        "  print(int_to_text(x))\n",
        "  print('\\nOUTPUT')\n",
        "  print(int_to_text(y))"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Example\n",
            "\n",
            "INPUT\n",
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You\n",
            "\n",
            "OUTPUT\n",
            "irst Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You \n",
            "\n",
            "\n",
            "Example\n",
            "\n",
            "INPUT\n",
            "are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you \n",
            "\n",
            "OUTPUT\n",
            "re all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you k\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "45mIdXmB1hk4"
      },
      "source": [
        "# MAKE TRAINING BATCHES\n",
        "\n",
        "BATCH_SIZE = 64\n",
        "VOCAB_SIZE = len(vocab)  #vovan is number of unique character\n",
        "EMBEDDING_DIM = 256\n",
        "RNN_UNITS = 1024\n",
        "\n",
        "BUFFER_SIZE = 10000   #shuffle this size of sequence intead of entire.\n",
        "\n",
        "data = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder = True)\n",
        "\n"
      ],
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u0a_EBTS3WzJ"
      },
      "source": [
        "**MODEL**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1dgF2lrG3T-8",
        "outputId": "1e2b739e-d115-43aa-9a8d-3ef97481479e"
      },
      "source": [
        "# will make a function\n",
        "\n",
        "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
        "  model = tf.keras.Sequential([\n",
        "                               tf.keras.layers.Embedding(vocab_size, embedding_dim, \n",
        "                                                         batch_input_shape = [batch_size, None]),\n",
        "                               tf.keras.layers.LSTM(rnn_units,\n",
        "                                                    return_sequences = True,\n",
        "                                                    stateful = True,\n",
        "                                                    recurrent_initializer = 'glorot_uniform'),\n",
        "                               tf.keras.layers.Dense(vocab_size)\n",
        "  ])\n",
        "\n",
        "  return model\n",
        "\n",
        "model = build_model(VOCAB_SIZE, EMBEDDING_DIM, RNN_UNITS, BATCH_SIZE)\n",
        "model.summary()"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_2 (Embedding)      (64, None, 256)           16640     \n",
            "_________________________________________________________________\n",
            "lstm_2 (LSTM)                (64, None, 1024)          5246976   \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (64, None, 65)            66625     \n",
            "=================================================================\n",
            "Total params: 5,330,241\n",
            "Trainable params: 5,330,241\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pu7jW3UkAW1C"
      },
      "source": [
        "**LOSS FUNCTION**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oBpQap9Q8jqU",
        "outputId": "3e99424e-0179-4fa5-e886-d53d600e4b81"
      },
      "source": [
        "for input_example_batch, target_example_batch in data.take(1):\n",
        "  example_batch_predictions = model(input_example_batch)\n",
        "  print(example_batch_predictions.shape)"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(64, 100, 65)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4xN3sZLJBEB8",
        "outputId": "c9c598ef-0060-4f7a-95f1-f80e0d0761d5"
      },
      "source": [
        "# see prediction in 64 array of array \n",
        "print(len(example_batch_predictions))\n",
        "print(example_batch_predictions)"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "64\n",
            "tf.Tensor(\n",
            "[[[-3.62738974e-05 -3.26399948e-03 -2.11016973e-03 ... -2.81044049e-04\n",
            "    1.82781753e-03 -1.60905148e-03]\n",
            "  [ 4.77091968e-03 -1.04946271e-02  8.58562998e-03 ...  5.99420280e-04\n",
            "    8.38554557e-03  3.38657899e-03]\n",
            "  [ 7.02647376e-04 -3.73084075e-03  1.17671993e-02 ... -5.91073302e-04\n",
            "    1.02513414e-02  3.70128616e-03]\n",
            "  ...\n",
            "  [ 7.28499424e-03 -7.00466847e-03  1.28159509e-03 ...  1.25183375e-03\n",
            "   -2.43848586e-03 -3.87968216e-03]\n",
            "  [ 5.95462834e-03 -1.06190527e-02  3.08870873e-03 ...  1.56158453e-03\n",
            "   -8.22332129e-03 -5.36829326e-03]\n",
            "  [ 4.78641037e-03 -1.37426145e-02  4.68015485e-03 ...  1.69402757e-03\n",
            "   -1.21643087e-02 -7.03235902e-03]]\n",
            "\n",
            " [[-1.30369712e-03  6.83549326e-03 -6.20381848e-04 ... -1.16400234e-03\n",
            "    2.00667419e-05 -2.70093884e-03]\n",
            "  [ 2.05996563e-03  8.59704707e-03 -4.37307026e-04 ... -2.76869582e-03\n",
            "   -3.41496989e-03  2.99177482e-03]\n",
            "  [ 2.77748005e-03  8.27050488e-03 -4.80622344e-04 ... -4.97276150e-03\n",
            "   -3.96913104e-03  1.03526819e-03]\n",
            "  ...\n",
            "  [ 5.55858715e-03 -9.43516009e-03  7.52231292e-03 ...  2.54099770e-03\n",
            "   -5.00649949e-05 -3.44374566e-03]\n",
            "  [ 4.00929712e-03 -1.70524651e-03  1.41773671e-02 ... -1.11511195e-04\n",
            "    1.45770516e-03 -4.51862440e-03]\n",
            "  [ 6.43321918e-03 -2.77325301e-03  1.45104881e-02 ... -2.20736186e-03\n",
            "    1.02392340e-03 -5.31756785e-03]]\n",
            "\n",
            " [[ 6.05868036e-03  5.02347946e-04  4.09204606e-03 ...  5.30497776e-03\n",
            "    7.37314904e-03  2.55250349e-03]\n",
            "  [ 3.21530597e-03 -1.60802156e-05  2.65910407e-03 ... -3.88419419e-03\n",
            "    3.31914192e-03 -3.78566305e-03]\n",
            "  [ 4.65877075e-03  3.08963208e-05  2.32622726e-03 ... -7.43493764e-03\n",
            "   -5.70395309e-03  2.49913754e-03]\n",
            "  ...\n",
            "  [ 3.72040807e-03 -3.44253215e-03  1.16315894e-02 ... -5.04016364e-03\n",
            "    1.19309109e-02 -9.43967840e-04]\n",
            "  [ 7.01042823e-03  1.56401249e-04  1.41894054e-02 ... -5.21177892e-03\n",
            "    9.30175278e-03 -1.58471311e-03]\n",
            "  [ 1.17584877e-02  1.59383414e-03  1.47614256e-02 ...  1.74267835e-03\n",
            "    1.43311135e-02  7.67016842e-04]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[-6.77924661e-04  4.30677505e-03  5.71007654e-03 ... -1.37795473e-03\n",
            "    1.48074620e-03 -2.92302039e-03]\n",
            "  [-1.77720073e-03 -1.63112569e-03 -1.27027591e-03 ...  1.78961805e-03\n",
            "   -1.12139678e-03 -5.55509515e-03]\n",
            "  [ 8.45890376e-04  1.60924252e-03  4.77429572e-03 ...  1.75061671e-03\n",
            "   -3.42397555e-03 -5.55846887e-03]\n",
            "  ...\n",
            "  [ 5.98714221e-03  2.67510465e-03  1.39021277e-02 ... -7.02917203e-03\n",
            "    8.99435766e-03 -5.27855242e-03]\n",
            "  [ 2.76816473e-03 -3.42091662e-03  7.99729116e-03 ... -1.81974168e-03\n",
            "    4.69423784e-03 -4.89963032e-03]\n",
            "  [ 1.95824471e-03  2.29849597e-03  1.22195268e-02 ... -1.78433792e-03\n",
            "    5.04109962e-03 -7.21980445e-03]]\n",
            "\n",
            " [[ 1.95353921e-03 -7.93242478e-04  1.27378467e-03 ... -1.04012387e-03\n",
            "   -4.60284267e-04 -2.80503626e-03]\n",
            "  [ 3.30352434e-03 -3.57402815e-03  4.68230201e-03 ... -1.64277095e-03\n",
            "    5.98955154e-03 -4.10991954e-03]\n",
            "  [ 1.86774123e-03  2.11814116e-03  9.87851527e-03 ... -1.97230559e-03\n",
            "    6.72193104e-03 -5.34497993e-03]\n",
            "  ...\n",
            "  [-1.84088794e-03 -3.83666717e-03  5.81719540e-03 ...  6.14059251e-03\n",
            "    3.38433147e-03 -4.53964714e-03]\n",
            "  [ 1.85000754e-04 -1.16297631e-02  4.74671321e-03 ...  2.82075163e-03\n",
            "   -2.04518365e-04 -6.30477630e-03]\n",
            "  [-1.41977356e-03 -7.89093878e-03  4.54082899e-03 ... -6.51161792e-03\n",
            "   -2.44127167e-03 -1.01172980e-02]]\n",
            "\n",
            " [[ 1.96590554e-03 -3.38192564e-04  3.61955259e-04 ... -5.07313060e-03\n",
            "   -8.82160291e-03  5.58691658e-03]\n",
            "  [ 3.61041958e-03  1.69302744e-03  6.12961221e-03 ... -3.59712821e-03\n",
            "   -1.08840894e-02  2.83387327e-03]\n",
            "  [-1.85629108e-03  2.98412004e-03  4.51245951e-03 ... -4.97058360e-03\n",
            "   -8.07040744e-03  4.98324807e-04]\n",
            "  ...\n",
            "  [ 2.48411088e-03  1.67198258e-03  9.94219445e-03 ...  1.89644366e-03\n",
            "   -5.42372349e-04 -1.11849513e-02]\n",
            "  [ 2.03682343e-03 -2.87975068e-03  6.38148142e-03 ...  4.97835339e-04\n",
            "    1.21957331e-04 -1.11361938e-02]\n",
            "  [-1.65588863e-03 -7.99548905e-03  1.98932551e-03 ...  4.51642834e-03\n",
            "   -4.89474565e-04 -1.05570154e-02]]], shape=(64, 100, 65), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EtNO9ZEwBEs-",
        "outputId": "4d9aa54c-25a9-47b5-b4c8-bb8019c9fc48"
      },
      "source": [
        "# see one prediction for example\n",
        "pred = example_batch_predictions[0]\n",
        "print(len(pred))\n",
        "print(pred)\n"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100\n",
            "tf.Tensor(\n",
            "[[-3.6273897e-05 -3.2639995e-03 -2.1101697e-03 ... -2.8104405e-04\n",
            "   1.8278175e-03 -1.6090515e-03]\n",
            " [ 4.7709197e-03 -1.0494627e-02  8.5856300e-03 ...  5.9942028e-04\n",
            "   8.3855456e-03  3.3865790e-03]\n",
            " [ 7.0264738e-04 -3.7308407e-03  1.1767199e-02 ... -5.9107330e-04\n",
            "   1.0251341e-02  3.7012862e-03]\n",
            " ...\n",
            " [ 7.2849942e-03 -7.0046685e-03  1.2815951e-03 ...  1.2518337e-03\n",
            "  -2.4384859e-03 -3.8796822e-03]\n",
            " [ 5.9546283e-03 -1.0619053e-02  3.0887087e-03 ...  1.5615845e-03\n",
            "  -8.2233213e-03 -5.3682933e-03]\n",
            " [ 4.7864104e-03 -1.3742615e-02  4.6801548e-03 ...  1.6940276e-03\n",
            "  -1.2164309e-02 -7.0323590e-03]], shape=(100, 65), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yYjaKLZ9BE2o",
        "outputId": "30ecce0f-a9ef-4604-975e-fa059029c37d"
      },
      "source": [
        "# see prediction at first time step\n",
        "#probability of each character occuring next\n",
        "time_pred = pred[0]\n",
        "print(len(time_pred))\n",
        "print(time_pred)"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "65\n",
            "tf.Tensor(\n",
            "[-3.6273897e-05 -3.2639995e-03 -2.1101697e-03  5.2232072e-03\n",
            " -1.8209992e-03  2.3395782e-03  4.2266655e-03  1.1061723e-03\n",
            " -2.4870788e-03  5.1675751e-03 -1.5849890e-03  1.5781400e-03\n",
            "  5.8703860e-03  6.1892741e-04 -5.9137377e-04  5.3535524e-04\n",
            "  3.9452673e-03  1.8023144e-03 -2.1675143e-03 -1.5955428e-03\n",
            " -6.6801966e-03  3.5678402e-03 -2.7283528e-03 -1.4853654e-03\n",
            " -4.7636968e-03  1.0023562e-04  3.9076163e-03 -9.0568181e-04\n",
            "  7.1107422e-04  5.5392403e-03  4.2444989e-04 -7.9077622e-03\n",
            "  3.1778754e-03  1.8600707e-03  7.1335485e-04  1.4725857e-03\n",
            "  4.7493684e-03 -1.2919090e-03 -3.0939313e-04 -2.2662033e-03\n",
            "  1.7559600e-04 -1.2480476e-03  2.7443704e-03  1.0931890e-03\n",
            "  2.0495008e-03 -3.0489557e-03 -2.3698714e-03 -4.1487329e-03\n",
            " -7.9927396e-04  4.8768311e-04  2.9980141e-04  3.5392428e-03\n",
            "  6.6022137e-03  6.1414484e-03 -5.4399497e-03  2.9606712e-03\n",
            " -9.7407268e-05 -9.4529637e-04  1.9215900e-03  2.1577836e-03\n",
            " -3.0165285e-04 -1.5285509e-03 -2.8104405e-04  1.8278175e-03\n",
            " -1.6090515e-03], shape=(65,), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "mY-SvSD3D7fy",
        "outputId": "bcc540de-c1bf-4f16-ed0e-482276ad6f30"
      },
      "source": [
        "#to see the predicted characters\n",
        "\n",
        "sample_indices = tf.random.categorical(pred, num_samples=1)\n",
        "\n",
        "sample_indices = np.reshape(sample_indices, (1,-1))[0]\n",
        "predicted_chars = int_to_text(sample_indices)\n",
        "\n",
        "predicted_chars"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"cPy ?&a:U$Vv-\\n.EB-\\nQcn&,OI,z;vYGoj!awdFt'T\\nzteuZqp!VG?MBaVMF;OnGeqOmTESfQw!fXX 3wPTc'SFoKIlr!BjYUp$S\""
            ]
          },
          "metadata": {},
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bRx8TrVLEtA1"
      },
      "source": [
        "# now create a loss function\n",
        "\n",
        "def loss(labels, logits):\n",
        "  return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n"
      ],
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kY17_EqkFq9_"
      },
      "source": [
        "**MODEL COMPILE**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nFGwe3EYFpg3"
      },
      "source": [
        "model.compile(optimizer = 'adam', loss = loss)"
      ],
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TddyRroyGEYu"
      },
      "source": [
        "**Creating Checkpoints**\n",
        "\n",
        "save checkpoints as model trains, we cam load model from checkpoint and continue training it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-L0lYiLWGcm8"
      },
      "source": [
        "# directory to save checkpoints\n",
        "\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "\n",
        "#name of checkpoint file\n",
        "\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, 'ckpt_{epoch}')\n",
        "\n",
        "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath = checkpoint_prefix,\n",
        "    save_weights_only = True)"
      ],
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CGgX383PHkom"
      },
      "source": [
        "**Training**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BoKQKnvVIIU6",
        "outputId": "fd1161cd-d47c-4426-81aa-36e9bbc948e5"
      },
      "source": [
        "history = model.fit(data, epochs = 1, callbacks = [checkpoint_callback])"
      ],
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "172/172 [==============================] - 33s 174ms/step - loss: 2.5806\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CsycTMXiIX6a"
      },
      "source": [
        "**LOADING MODEL**\n",
        "\n",
        "rebuild for batch size 1, so we can feed one sentence to test the prediction."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DTOnjA2iHnhH"
      },
      "source": [
        "model = build_model(VOCAB_SIZE, EMBEDDING_DIM, RNN_UNITS, batch_size = 1)   #in batch size 1"
      ],
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-3-abc-6J3uh"
      },
      "source": [
        "#to find the checkpoints \n",
        "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
        "model.build(tf.TensorShape([1, None]))\n"
      ],
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 129
        },
        "id": "Xm8XuEgaKVys",
        "outputId": "b350f55e-6968-40b3-ec2c-6ec977e055b1"
      },
      "source": [
        "#we can also load specified checkpoint\n",
        "\n",
        "checkpoint_num = 10\n",
        "model.load_weights(tf.train.load_checkpoint('./content/training/checkpoints/ckpt_'+ str(checkpoint_num))\n",
        "model.build(tf.TensorShape([1, None]))"
      ],
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-90-bfa2333ad90f>\"\u001b[0;36m, line \u001b[0;32m5\u001b[0m\n\u001b[0;31m    model.build(tf.TensorShape([1, None]))\u001b[0m\n\u001b[0m        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Wpg-dp7LsVl"
      },
      "source": [
        "#function for any length of text\n",
        "\n",
        "def generate_text(model, start_string):\n",
        "\n",
        "  num_generate = 800     #num of char to generate\n",
        "\n",
        "  #convert string to num(vectorization)\n",
        "  input_eval = [char2idx[s] for s in start_string]\n",
        "  input_eval = tf.expand_dims(input_eval, 0)\n",
        "\n",
        "  #empty list to store string\n",
        "  text_generated = []\n",
        "\n",
        "  temperature = 1.0\n",
        "\n",
        "  model.reset_states()\n",
        "  for i in range(num_generate):\n",
        "    predictions = model(input_eval)\n",
        "    #remove the batch dimension\n",
        "    predictions = tf.squeeze(predictions, 0)\n",
        "\n",
        "    predictions = predictions/temperature\n",
        "    predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
        "\n",
        "    #pass predicted output as next input to the model\n",
        "    #along with previous hidden state\n",
        "\n",
        "    input_eval = tf.expand_dims([predicted_id], 0)\n",
        "    text_generated.append(idx2char[predicted_id])\n",
        "\n",
        "  return (start_string + ''.join(text_generated))"
      ],
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "83dhmUVqQeAB",
        "outputId": "c024ffd8-2f03-4186-891b-9972410b9202"
      },
      "source": [
        "inp = input(\"Type a startig string:\")\n",
        "print(generate_text(model, inp))"
      ],
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Type a startig string:romeo\n",
            "romeou, thy and meney\n",
            "You forkaun ann. I'dading hard bagt ince ay hilp?\n",
            "\n",
            "PARTES:\n",
            "The withtur apsod, dor, benet; Buenthare hest akn ve ang Youed,\n",
            "Be vand ane you puru, to meme them\n",
            "your murpeand.\n",
            "\n",
            "ARLIS HASNILK:\n",
            "I pade read astsimon ;\n",
            "Chaign beived,\n",
            "Sele steirird-st?\n",
            "Thet so aio duth Slexand aserpprem's!\n",
            "Benouts whine of ugpricand mird ruby yium, thare's prear.\n",
            "\n",
            "AREMIO:\n",
            "Ss mard ne soust fores ale and?\n",
            "\n",
            "MALTERRES:\n",
            "Cithoud, icatist thy pamiis bater like.\n",
            "\n",
            "COMETCARCEO:\n",
            "Banour I than shalv at wille noon.\n",
            "\n",
            "LERCERTaOing atlond of unwiot nis.\n",
            "\n",
            "FERENIS:\n",
            "Ay wo the fresting, mensid winte yoo whall d the sadovt,ssencloutter afly ryoit\n",
            "Sreafur Camonts--urntly wers meaded;\n",
            "Herer miecy the gamknous yot rrey\n",
            "But be not.\n",
            "BAO fopt:\n",
            "Aft'ss eperty:\n",
            "IJk hid shey sher fhip of ast to bidit mante, sen hey ant thou nal\n"
          ]
        }
      ]
    }
  ]
}